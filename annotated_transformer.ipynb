{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNSNGy1rviVadm/mVERAuV4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/manikTharaka/annotated-transformer-impl/blob/main/annotated_transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dChcWA7crYO9"
      },
      "outputs": [],
      "source": [
        "!pip install -q torchdata==0.3.0 torchtext==0.12 spacy==3.2 altair GPUtil\n",
        "!python -m spacy download de_core_news_sm\n",
        "!python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from os.path import exists\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn.functional import log_softmax, pad\n",
        "import math\n",
        "import copy\n",
        "import time\n",
        "from torch.optim.lr_scheduler import LambdaLR\n",
        "import pandas as pd\n",
        "import altair as alt\n",
        "from torchtext.data.functional import to_map_style_dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "import torchtext.datasets as datasets\n",
        "import spacy\n",
        "import GPUtil\n",
        "import warnings\n",
        "from torch.utils.data.distributed import DistributedSampler\n",
        "import torch.distributed as dist\n",
        "import torch.multiprocessing as mp\n",
        "from torch.nn.parallel import DistributedDataParallel as DDP\n",
        "\n",
        "\n",
        "# Set to False to skip notebook execution (e.g. for debugging)\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "RUN_EXAMPLES = True"
      ],
      "metadata": {
        "id": "DvTsUMJHt1uc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def is_interactive_notebook():\n",
        "    return __name__ == \"__main__\"\n",
        "\n",
        "\n",
        "def show_example(fn, args=[]):\n",
        "    if __name__ == \"__main__\" and RUN_EXAMPLES:\n",
        "        return fn(*args)\n",
        "\n",
        "\n",
        "def execute_example(fn, args=[]):\n",
        "    if __name__ == \"__main__\" and RUN_EXAMPLES:\n",
        "        fn(*args)\n",
        "\n",
        "\n",
        "class DummyOptimizer(torch.optim.Optimizer):\n",
        "    def __init__(self):\n",
        "        self.param_groups = [{\"lr\": 0}]\n",
        "        None\n",
        "\n",
        "    def step(self):\n",
        "        None\n",
        "\n",
        "    def zero_grad(self, set_to_none=False):\n",
        "        None\n",
        "\n",
        "\n",
        "class DummyScheduler:\n",
        "    def step(self):\n",
        "        None"
      ],
      "metadata": {
        "id": "r3V0oF2HuD64"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderDecoder(nn.Module):\n",
        "\n",
        "    def __init__(self,encodermdecoder,src_embed,tgt_embed,generator):\n",
        "        super(EncoderDecoder,self).__init__()\n",
        "        self.encoder=encoder\n",
        "        self.decoder=decoder\n",
        "        self.src_embed=src_embed\n",
        "        self.tgt_embed=tgt_embed\n",
        "        self.generator=generator\n",
        "\n",
        "    def forward(self,src,tgt,src_mask,tgt_mask):\n",
        "        return self.decode(self.encode(src,src_mask),src_mask,tgt,tgt_mask)\n",
        "\n",
        "    def encode(self,src,src_mask):\n",
        "        encoded = self.encoder(self.src_embed(src),src_mask)\n",
        "        return encoded\n",
        "\n",
        "    def decode(self,memory,src_mask,tgt,tgt_mask):\n",
        "        decoded = self.dcoder(self.tgt_embed(tgt),memory,src_mask,tgt_mask)\n",
        "\n",
        "        return decoded"
      ],
      "metadata": {
        "id": "hx3FXSdQutui"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Generator(nn.Module):\n",
        "\n",
        "    def __init__(self,d_model,vocab) -> None:\n",
        "        super(Generator,self).__init__()\n",
        "        self.proj = nn.Linear(d_model,vocab)\n",
        "\n",
        "    def forward(self,x):\n",
        "        return log_softmax(self.proj(x),dim=1)"
      ],
      "metadata": {
        "id": "rbIvZhpVyg69"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clones(module,N):\n",
        "    \"\"\"Make identical copies of the module\"\"\"\n",
        "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "\n",
        "    def __init__(self,layer,N):\n",
        "        super(Encoder,self).__init__()\n",
        "        self.layers = clones(layer,N)\n",
        "        self.norm = LayerNorm(layer.size)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x,mask)\n",
        "\n",
        "        return self.norm(x)"
      ],
      "metadata": {
        "id": "W3l6RF11y6oD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LayerNorm(nn.Module):\n",
        "\n",
        "    def __init__(self,features,eps=1e-6):\n",
        "        super(LayerNorm,self).__init__()\n",
        "        self.a_2 = nn.Parameter(torch.ones(features))\n",
        "        self.b_2 = nn.Parameter(torch.zeros(features))\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self,x):\n",
        "        mean = x.mean(-1,keepdim=True)\n",
        "        std  = x.std(-1,keepdim=True)\n",
        "\n",
        "        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2"
      ],
      "metadata": {
        "id": "L9WGFQvC2Q7y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SubLayerConnection(nn.Module):\n",
        "\n",
        "    def __init__(self,size,dropout):\n",
        "        super().__init__()\n",
        "        self.norm = LayerNorm(size)\n",
        "        self.droput = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, sublayer):\n",
        "\n",
        "        return x + self.dropout(sublayer(self.norm(x)))"
      ],
      "metadata": {
        "id": "EziVFh2s0wyQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "\n",
        "    def __init__(self,size, self_attn,feed_forward,dropout):\n",
        "        super().__init__()\n",
        "        self.self_attention = self_attn\n",
        "        self.feed_forward = feed_forward\n",
        "        self.sublayer =  clones(SubLayerConnection(size,dropout),2)\n",
        "        self.size = size\n",
        "\n",
        "    def forward(self,x,mask):\n",
        "        x = self.sublayer[0](x,lambda x:self.self_attention(x,x,x,mask))\n",
        "\n",
        "        return self.sublayer[1](x,self.feed_forward)"
      ],
      "metadata": {
        "id": "keevWLvTNl2s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "\n",
        "    def __init__(self,layer,N):\n",
        "        super().__init__()\n",
        "        self.layers = clones(layer,N)\n",
        "        self.norm = LayerNorm(layer.size)\n",
        "\n",
        "    def forward(self,x,memory,src_mask,tgt_mask):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x,memory,src_mask,tgt_mask)\n",
        "\n",
        "        return self.norm(x)"
      ],
      "metadata": {
        "id": "YH6RCujsOkAM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "\n",
        "    def __init__(self,size,self_attention,src_attention,feed_forward,dropout):\n",
        "        self.size = size\n",
        "        self.self_attention = self_attention\n",
        "        self.src_attention = src_attention\n",
        "        self.feed_forward = feed_forward\n",
        "        self.sublayer = clones(SubLayerConnection(size,dropout),3)\n",
        "\n",
        "    def forward(self,x,memory,src_mask,tgt_mask):\n",
        "        m = memory\n",
        "        x = self.sublayer[0](x, lambda x:self.self_attention(x,x,x,tgt_mask))\n",
        "        x = self.sublayer[1](x, lambda x:self.src_attention(x,m,m,src_mask))\n",
        "        return self.sublayer[2](x,self.feed_forward)"
      ],
      "metadata": {
        "id": "PpJpVijpQNFv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9kzvTFCyQCzp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}